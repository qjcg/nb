{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "Useful for getting data from the web when a formal API isn't exposed by a website, or the API doesn't meet your needs.\n",
    "\n",
    "## Two Approaches\n",
    "\n",
    "- Basic web client (ex: `requests` library)\n",
    "    - Used for STATIC web pages rendered on the SERVER SIDE (before content arrives in your browser)\n",
    "- Headless browser piloted by Python\n",
    "    - Used when JavaScript is used to assemble a web page DYNAMICALLY on the CLIENT/BROWSER SIDE\n",
    "    \n",
    "## Web Technologies\n",
    "\n",
    "- HyperText Markup Language (HTML): Describes a web page document (headers, paragraphs, links, images, etc)\n",
    "- Cascading Style Sheets (CSS): Styling code for HTML (colors, font sizes, etc)\n",
    "    - Uses the concept of \"selectors\" to specify HTML elements to target\n",
    "        - For a fun tutorial on CSS selectors: <https://flukeout.github.io/>\n",
    "- JavaScript: Code that implements behavior for web applications (ex: when I click on this, do that, etc)\n",
    "- JavaScript Object Notation (JSON): A markup format that describes data. One of the primary formats returned by popular web APIs (Instagram, FaceBook, ICNDB, etc)\n",
    "- HyperText Transfer Protocol (HTTP): The application-level network protocol used by browsers and other web clients to retrieve data from web servers.\n",
    "    - Has actions like:\n",
    "        - `GET`: Retrieve data from a server.\n",
    "        - `POST`: Upload / create new data on the server.\n",
    "        - `PUT`: Modify existing data on a server.\n",
    "        - `DELETE`: Delete data from a server.\n",
    "    \n",
    "## Libraries and Tools\n",
    "\n",
    "### Static Scraping\n",
    "- `requests`: The most popular and easy to use Python HTTP client library\n",
    "- `BeautifulSoup`: Useful for parsing HTML / XML data (often obtained using `requests`) and extracting only the data you're interested in\n",
    "- `feedparser`: Like BeautifulSoup, but focused on RSS / ATOM feed data\n",
    "\n",
    "### Dynamic Scraping\n",
    "- `selenium`: A popular, cross-language framework for piloting headless web browsers\n",
    "- `splinter`: A convenient, higher level interface to Selenium in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Review: Static Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our HTTP status code indicates success! 200\n"
     ]
    }
   ],
   "source": [
    "# CODE REVIEW: Static Scraping\n",
    "\n",
    "# Import common third-party libraries used for web scraping.\n",
    "## requests: A popular HTTP client library.\n",
    "## bs4: Beautiful Soup 4, Useful for parsing HTML / XML data\n",
    "##      (often obtained using requests) \n",
    "##      and extracting only the data you're interested in\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Do an HTTP GET request for the Berkshire Hathaway URL,\n",
    "# and store the result object from that request in the \"response\"\n",
    "# variable.\n",
    "response = requests.get(\"http://www.berkshirehathaway.com/\")\n",
    "\n",
    "# NOTE: There are lots of interesting attributes and methods\n",
    "#       available on the response object. For example, e can check the\n",
    "#       response.status_code to see whether our request succeeded.\n",
    "#       Reference: https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\n",
    "if 200 <= response.status_code <= 400:\n",
    "    print(f\"Our HTTP status code indicates success! {response.status_code}\")\n",
    "else:\n",
    "    print(f\"Our HTTP status code indicates an error! {response.status_code}\")\n",
    "\n",
    "\n",
    "# Parse the HTML content from the response into a format we can\n",
    "# more easily query and extract data from.\n",
    "soup = BeautifulSoup(response.content)\n",
    "\n",
    "# Select the HTML data we're interested in.\n",
    "# In the case of the BH website, imagine we wanted:\n",
    "#   - the TEXT of each link on the main page\n",
    "#   - the URL for each link on the main page\n",
    "\n",
    "# We use \"CSS selector\" notation with the soup.select method\n",
    "# to extract this data.\n",
    "links = soup.select('li a')\n",
    "\n",
    "# To STORE the extracted data for exploration and further use,\n",
    "# we need to decide on a data structure.\n",
    "# Because the data we're interested in comes in pairs (text of link, URL),\n",
    "# a dictionary data structure makes sense here.\n",
    "# To create our dictionary, we use a DICT COMPREHENSION.\n",
    "# We populate our dict with pairs (only where there is an href attribute):\n",
    "# - key: the first string in each link's contents\n",
    "# - value: the URL corresponding to each link\n",
    "links_dict = {link.contents[0]: link.attrs['href'] \n",
    "              for link in links if link.attrs.get('href')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Message From Warren E. Buffett: message.html\n",
      "News Releases: news/2019news.html\n",
      "Annual & Interim Reports: reports.html\n",
      "Warren Buffett's Letters to Berkshire \r\n",
      "        Shareholders: letters/letters.html\n",
      "Special Letters From Warren & Charlie RE:Past, Present and Future: SpecialLetters/WEBCTMLtr.html\n",
      "Charlie Munger's Letters to Wesco Shareholders: wesco/WescoHome.html\n",
      "Link \r\n",
      "        to SEC Filings: http://www.sec.gov/cgi-bin/browse-edgar?company=berkshire+hathaway&match=&CIK=&filenum=&State=&Country=&SIC=&owner=exclude&Find=Find+Companies&action=getcompany\n",
      "Annual Meeting Information: sharehold.html\n",
      "Links to Berkshire Subsidiary \r\n",
      "        Companies: subs/sublinks.html\n",
      "Celebrating 50 Years of a Profitable Partnership: https://www.ebay.com/itm/173723452844\n",
      "Corporate Governance: govern/govern.html\n",
      "Comparative Rights and Relative Prices of \r\n",
      "        Class A and B Stock: compab.pdf\n",
      "Sustainability: sustainability/sustainability.html\n",
      "Berkshire Activewear \r\n",
      "        : http://www.berkshirewear.com\n",
      "Owner's Manual: ownman.pdf\n",
      "Letters from Warren E. Buffett \r\n",
      "        Regarding Pledges to Make Gifts of Berkshire Stock: donate/webdonat.html\n"
     ]
    }
   ],
   "source": [
    "# Here we do SOMETHING with our data: print it.\n",
    "# We could also do other things --- \n",
    "# download all URLS, render a report template, \n",
    "# send a notification if a value changes, etc!\n",
    "for text, url in links_dict.items():\n",
    "    print(f\"{text}: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE: Scraping Static Websites\n",
    "\n",
    "For ONE of the following websites:\n",
    "\n",
    "- https://drw.com/\n",
    "- https://gossetx.com/\n",
    "- https://en.wikipedia.org/wiki/Main_Page\n",
    "\n",
    "use `requests` and `BeautifulSoup` to retrieve and parse the site, and return a list of the *img* (image) elements. Example code:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get(YOUR_URL)\n",
    "soup = BeautifulSoup(res.text)\n",
    "images = soup.select('img')\n",
    "print(images)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Review: Retrieving data from a JSON API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE REVIEW: Retrieving data from a JSON API\n",
    "\n",
    "# import the popular HTTP client library.\n",
    "import requests\n",
    "\n",
    "# Define a function for retrieving jokes so that we can REUSE and/or test it easily.\n",
    "def icndb(num_jokes=3):\n",
    "    \"\"\"Retrieve random jokes from the ICNDB API.\n",
    "    \n",
    "    Reference: http://www.icndb.com/api/\n",
    "    \"\"\"\n",
    "    # Use an HTTP GET request to retrieve some random jokes.\n",
    "    res = requests.get(f\"http://api.icndb.com/jokes/random/{num_jokes}\")\n",
    "    \n",
    "    # Get the JSON data from the response object.\n",
    "    data = res.json()\n",
    "    \n",
    "    # Use a LIST COMPREHENSION to create a list of ONLY the joke strings.\n",
    "    # Return that list from our function.\n",
    "    return [v['joke'] for v in data['value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chuck Norris is not Politically Correct. He is just Correct. Always.',\n",
       " \"The quickest way to a man's heart is with Chuck Norris' fist.\",\n",
       " 'James Cameron wanted Chuck Norris to play the Terminator. However, upon reflection, he realized that would have turned his movie into a documentary, so he went with Arnold Schwarzenegger.',\n",
       " \"&quot;Brokeback Mountain&quot; is not just a movie. It's also what Chuck Norris calls the pile of dead ninjas in his front yard.\",\n",
       " \"When Chuck Norris throws exceptions, it's across the room.\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icndb(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping a Dynamic Website using a Headless Web Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
